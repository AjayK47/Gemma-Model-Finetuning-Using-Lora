{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Gemma Model on Question Answering Dataset","metadata":{}},{"cell_type":"markdown","source":"### Install Libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q --upgrade datasets\n!pip install -q --upgrade transformers\n!pip install -q --upgrade peft\n!pip install -q --upgrade trl\n!pip install -q bitsandbytes\n!pip install -q accelerate\n\n\n# for logging and visualizing training progress\n!pip install -q tensorboard","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:00:16.459188Z","iopub.execute_input":"2024-03-18T13:00:16.459961Z","iopub.status.idle":"2024-03-18T13:02:13.691798Z","shell.execute_reply.started":"2024-03-18T13:00:16.459914Z","shell.execute_reply":"2024-03-18T13:02:13.690428Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.1 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.1 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\nimport transformers\nimport torch\nfrom trl import SFTTrainer\nfrom peft import LoraConfig,PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    logging,\n    GemmaTokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:03:11.088728Z","iopub.execute_input":"2024-03-18T13:03:11.089112Z","iopub.status.idle":"2024-03-18T13:03:29.985264Z","shell.execute_reply.started":"2024-03-18T13:03:11.089076Z","shell.execute_reply":"2024-03-18T13:03:29.984502Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-03-18 13:03:20.469825: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-18 13:03:20.469933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-18 13:03:20.601266: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Hugging face Token","metadata":{}},{"cell_type":"markdown","source":"Attach hugging face token to this notebook in add-ons >> secrets","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nos.environ[\"HF_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:04:52.720558Z","iopub.execute_input":"2024-03-18T13:04:52.721432Z","iopub.status.idle":"2024-03-18T13:04:52.894008Z","shell.execute_reply.started":"2024-03-18T13:04:52.721398Z","shell.execute_reply":"2024-03-18T13:04:52.893220Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"import the Dataset you formatted from your Hugging repo","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"Ajayk/indian_history_formatted\" \ndataset = load_dataset(dataset_name, split=\"train[0:1000]\")\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:09:48.969698Z","iopub.execute_input":"2024-03-18T13:09:48.970511Z","iopub.status.idle":"2024-03-18T13:09:52.004597Z","shell.execute_reply.started":"2024-03-18T13:09:48.970480Z","shell.execute_reply":"2024-03-18T13:09:52.003821Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c0abaa04aa44a19143be0a0d88a73d"}},"metadata":{}},{"name":"stderr","text":"Downloading data: 100%|██████████| 4.77M/4.77M [00:00<00:00, 20.1MB/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e29d6ad9194ac9b602415a633ab56c"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['text'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"I am training only First 1000 of Data to showcase the demonstration","metadata":{}},{"cell_type":"code","source":"# define some variables - model names\nmodel_name = \"google/gemma-2b-it\" \nnew_model = \"gemma-ft-indian-history\" ## replace with name you prefer for fine tuned model","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:09:40.293142Z","iopub.execute_input":"2024-03-18T13:09:40.294042Z","iopub.status.idle":"2024-03-18T13:09:40.298102Z","shell.execute_reply.started":"2024-03-18T13:09:40.294007Z","shell.execute_reply":"2024-03-18T13:09:40.297158Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"you can import General 2B model or Gemma 7B models ,I am import Gemma 2B instruction model here","metadata":{}},{"cell_type":"code","source":"# LoRA parameters\nlora_r = 4\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n# Dropout probability for LoRA layers\nlora_dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:12:00.770314Z","iopub.execute_input":"2024-03-18T13:12:00.770710Z","iopub.status.idle":"2024-03-18T13:12:00.775179Z","shell.execute_reply.started":"2024-03-18T13:12:00.770683Z","shell.execute_reply":"2024-03-18T13:12:00.774116Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# bitsandbytes parameters\n# Activate 4-bit precision base model loading\nuse_4bit = True\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n# Activate nested quantization for 4-bit base models\nuse_nested_quant = False","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:12:07.267525Z","iopub.execute_input":"2024-03-18T13:12:07.268312Z","iopub.status.idle":"2024-03-18T13:12:07.272756Z","shell.execute_reply.started":"2024-03-18T13:12:07.268277Z","shell.execute_reply":"2024-03-18T13:12:07.271708Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Output directory for storage\noutput_dir = \"./indian-history-gemma-instructuion-finetuned\"","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:12:09.613269Z","iopub.execute_input":"2024-03-18T13:12:09.613641Z","iopub.status.idle":"2024-03-18T13:12:09.617902Z","shell.execute_reply.started":"2024-03-18T13:12:09.613614Z","shell.execute_reply":"2024-03-18T13:12:09.616984Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### TrainingArguments parameters","metadata":{}},{"cell_type":"code","source":"# Number of training epochs\nnum_train_epochs = 3\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n# Enable gradient checkpointing\ngradient_checkpointing = True\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"constant\"\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n# Save checkpoint every X updates steps\nsave_steps = 25\n# Log every X updates steps\nlogging_steps = 25","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:01.899714Z","iopub.execute_input":"2024-03-18T13:15:01.900070Z","iopub.status.idle":"2024-03-18T13:15:01.906933Z","shell.execute_reply.started":"2024-03-18T13:15:01.900041Z","shell.execute_reply":"2024-03-18T13:15:01.906105Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### SFT parameters","metadata":{}},{"cell_type":"code","source":"# Maximum sequence length to use\nmax_seq_length = 40 # None\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = True # False\n# Load the entire model on the GPU 0\n# device_map = {\"\": 0}\ndevice_map=\"auto\"","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:13:49.670883Z","iopub.execute_input":"2024-03-18T13:13:49.671242Z","iopub.status.idle":"2024-03-18T13:13:49.676193Z","shell.execute_reply.started":"2024-03-18T13:13:49.671212Z","shell.execute_reply":"2024-03-18T13:13:49.675200Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Load QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit, # Activates 4-bit precision loading\n    bnb_4bit_quant_type=bnb_4bit_quant_type, # nf4\n    bnb_4bit_compute_dtype=compute_dtype, # float16\n    bnb_4bit_use_double_quant=use_nested_quant, # False\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:07.208046Z","iopub.execute_input":"2024-03-18T13:15:07.208770Z","iopub.status.idle":"2024-03-18T13:15:07.214313Z","shell.execute_reply.started":"2024-03-18T13:15:07.208739Z","shell.execute_reply":"2024-03-18T13:15:07.213447Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"Setting BF16 to True\")\n        bf16 = True\n    else:\n        bf16 = False","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:11.491354Z","iopub.execute_input":"2024-03-18T13:15:11.492224Z","iopub.status.idle":"2024-03-18T13:15:11.499777Z","shell.execute_reply.started":"2024-03-18T13:15:11.492176Z","shell.execute_reply":"2024-03-18T13:15:11.498842Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=device_map\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name,\n                                          trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" ","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:16.387551Z","iopub.execute_input":"2024-03-18T13:15:16.387912Z","iopub.status.idle":"2024-03-18T13:15:38.949699Z","shell.execute_reply.started":"2024-03-18T13:15:16.387882Z","shell.execute_reply":"2024-03-18T13:15:38.948868Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a783f2b526634399832bedf372482ff6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57c2c7ffdb04d9687eca5e9ac236f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d5e773092e84acba6b1bddbe6dbc909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bbcdf25a7e4178bfea8307dd5a4f4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be998f9baf8453f8560b9b1e6710c4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c9c6775fdfd428dbc571b00e23fbdd2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895507cdc6ba4da08edeb85961703a95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"105bb550d519485ab361d7c155de5cbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b053c611900a44458246f6a70f1404b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"225edad331fa4ad5b22d792a61a26b6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf22b210566c4adc9d256e2dd9ce9b9d"}},"metadata":{}}]},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:41.905310Z","iopub.execute_input":"2024-03-18T13:15:41.905722Z","iopub.status.idle":"2024-03-18T13:15:41.914535Z","shell.execute_reply.started":"2024-03-18T13:15:41.905693Z","shell.execute_reply":"2024-03-18T13:15:41.913495Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n)\ntraining_arguments","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:15:53.206136Z","iopub.execute_input":"2024-03-18T13:15:53.206486Z","iopub.status.idle":"2024-03-18T13:15:53.217911Z","shell.execute_reply.started":"2024-03-18T13:15:53.206461Z","shell.execute_reply":"2024-03-18T13:15:53.217089Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=False,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_steps=None,\nevaluation_strategy=no,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=True,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=0.0002,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./indian-history-gemma-instructuion-finetuned/runs/Mar18_13-15-53_0a49bdea9770,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=25,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=constant,\nmax_grad_norm=0.3,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3,\noptim=paged_adamw_32bit,\noptim_args=None,\noutput_dir=./indian-history-gemma-instructuion-finetuned,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=8,\nper_device_train_batch_size=4,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard'],\nresume_from_checkpoint=None,\nrun_name=./indian-history-gemma-instructuion-finetuned,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=25,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_mps_device=False,\nwarmup_ratio=0.03,\nwarmup_steps=0,\nweight_decay=0.001,\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:13.287408Z","iopub.execute_input":"2024-03-18T13:16:13.287792Z","iopub.status.idle":"2024-03-18T13:16:14.234856Z","shell.execute_reply.started":"2024-03-18T13:16:13.287762Z","shell.execute_reply":"2024-03-18T13:16:14.234099Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f79c48d6b374581a948050bc36834b3"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Train model","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:16:49.088431Z","iopub.execute_input":"2024-03-18T13:16:49.089555Z","iopub.status.idle":"2024-03-18T13:27:52.889440Z","shell.execute_reply.started":"2024-03-18T13:16:49.089511Z","shell.execute_reply":"2024-03-18T13:27:52.888521Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1158' max='1158' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1158/1158 11:01, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>5.800200</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>3.248300</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.644000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.546900</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>2.362800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.244100</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>2.065000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.147000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>2.036500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.910600</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>1.917000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.787200</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>1.727400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.756300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>1.843200</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.540000</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>1.284900</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.342500</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>1.275900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.183800</td>\n    </tr>\n    <tr>\n      <td>525</td>\n      <td>1.228200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.171600</td>\n    </tr>\n    <tr>\n      <td>575</td>\n      <td>1.259100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.306200</td>\n    </tr>\n    <tr>\n      <td>625</td>\n      <td>1.289500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.229700</td>\n    </tr>\n    <tr>\n      <td>675</td>\n      <td>1.226000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.261900</td>\n    </tr>\n    <tr>\n      <td>725</td>\n      <td>1.302100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.271500</td>\n    </tr>\n    <tr>\n      <td>775</td>\n      <td>1.216300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.789300</td>\n    </tr>\n    <tr>\n      <td>825</td>\n      <td>0.780400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.780500</td>\n    </tr>\n    <tr>\n      <td>875</td>\n      <td>0.850800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.779800</td>\n    </tr>\n    <tr>\n      <td>925</td>\n      <td>0.771200</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.793800</td>\n    </tr>\n    <tr>\n      <td>975</td>\n      <td>0.795000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.847300</td>\n    </tr>\n    <tr>\n      <td>1025</td>\n      <td>0.833800</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.876400</td>\n    </tr>\n    <tr>\n      <td>1075</td>\n      <td>0.865900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.859100</td>\n    </tr>\n    <tr>\n      <td>1125</td>\n      <td>0.874000</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.848700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1158, training_loss=1.4901720291592297, metrics={'train_runtime': 663.2873, 'train_samples_per_second': 6.974, 'train_steps_per_second': 1.746, 'total_flos': 2204337359093760.0, 'train_loss': 1.4901720291592297, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"code","source":"## Save the model \ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:04.114984Z","iopub.execute_input":"2024-03-18T13:28:04.115959Z","iopub.status.idle":"2024-03-18T13:28:04.386082Z","shell.execute_reply.started":"2024-03-18T13:28:04.115924Z","shell.execute_reply":"2024-03-18T13:28:04.385267Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"## send the Model to Hugging face\ntrainer.push_to_hub(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:10.038900Z","iopub.execute_input":"2024-03-18T13:28:10.039753Z","iopub.status.idle":"2024-03-18T13:28:13.509891Z","shell.execute_reply.started":"2024-03-18T13:28:10.039720Z","shell.execute_reply":"2024-03-18T13:28:13.508945Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/14.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf88f9618ba04d8887de9a50f782ab3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901248e736ef4917bfdf9a15d6e67b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84225c3371f4e10bafc529272883595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"594fa017608a4e9d991b9c5d3e3cebf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1710767809.0a49bdea9770.34.0:   0%|          | 0.00/15.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7c37d6e84ed44299b19ee619725b1a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa271c057a60442dbe4cbc6a09b7073b"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Ajayk/indian-history-gemma-instructuion-finetuned/commit/66f2de9d3e4dc23cc0ff1d35d60b4f6628a0ffa6', commit_message='gemma-ft-indian-history', commit_description='', oid='66f2de9d3e4dc23cc0ff1d35d60b4f6628a0ffa6', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"# Send all results to tensorboard\n%load_ext tensorboard\n%tensorboard --logdir results/runs","metadata":{"execution":{"iopub.status.busy":"2024-03-18T13:28:17.166885Z","iopub.execute_input":"2024-03-18T13:28:17.167763Z","iopub.status.idle":"2024-03-18T13:28:23.700574Z","shell.execute_reply.started":"2024-03-18T13:28:17.167728Z","shell.execute_reply":"2024-03-18T13:28:23.699524Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]}]}